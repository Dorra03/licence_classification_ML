"""
Classification Model Building & Training
Implements multiple ML approaches for SPDX license classification

Approaches:
1. TEXT-BASED: TF-IDF + String Matching / Regex
2. MACHINE LEARNING: SVM, Random Forest, Gradient Boosting
3. EMBEDDINGS: BERT/CodeBERT
4. HYBRID: Combine ML + Rule-based detection
"""

import pandas as pd
import numpy as np
import os
import json
import pickle
import logging
import time
import warnings
from pathlib import Path

# ML Libraries
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, top_k_accuracy_score
)
from sklearn.preprocessing import StandardScaler
import scipy.sparse as sp

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LicenseClassifier:
    def __init__(self, features_dir, output_dir):
        self.features_dir = Path(features_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.models = {}
        self.results = {}
        
    def load_features(self):
        """Load preprocessed features"""
        logger.info("Loading preprocessed features...")
        
        self.X_train = sp.load_npz(self.features_dir / 'X_train.npz')
        self.X_test = sp.load_npz(self.features_dir / 'X_test.npz')
        
        self.y_train = pd.read_csv(self.features_dir / 'y_train.csv')['license_id']
        self.y_test = pd.read_csv(self.features_dir / 'y_test.csv')['license_id']
        
        # Load encoders and vectorizer
        with open(self.features_dir / 'encoders.pkl', 'rb') as f:
            self.encoders = pickle.load(f)
        
        with open(self.features_dir / 'vectorizer.pkl', 'rb') as f:
            self.vectorizer = pickle.load(f)
        
        logger.info(f"X_train shape: {self.X_train.shape}")
        logger.info(f"X_test shape: {self.X_test.shape}")
        logger.info(f"y_train samples: {len(self.y_train)}")
        logger.info(f"y_test samples: {len(self.y_test)}")
        
        return self.X_train, self.X_test, self.y_train, self.y_test
    
    def train_svm(self, kernel='rbf', C=1.0):
        """Train SVM classifier"""
        logger.info(f"Training SVM classifier (kernel={kernel}, C={C})...")
        start_time = time.time()
        
        model = SVC(kernel=kernel, C=C, verbose=0)
        model.fit(self.X_train, self.y_train)
        
        elapsed = time.time() - start_time
        logger.info(f"Training completed in {elapsed:.2f} seconds")
        
        self.models['SVM'] = model
        return model
    
    def train_random_forest(self, n_estimators=100, max_depth=20):
        """Train Random Forest classifier"""
        logger.info(f"Training Random Forest (n_estimators={n_estimators}, max_depth={max_depth})...")
        start_time = time.time()
        
        model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            n_jobs=-1,
            verbose=0,
            random_state=42
        )
        model.fit(self.X_train, self.y_train)
        
        elapsed = time.time() - start_time
        logger.info(f"Training completed in {elapsed:.2f} seconds")
        
        self.models['Random Forest'] = model
        return model
    
    def train_gradient_boosting(self, n_estimators=50, learning_rate=0.1):
        """Train Gradient Boosting classifier (skipped - too slow with 5K features)"""
        logger.info(f"Skipping Gradient Boosting - too computationally expensive with {self.X_train.shape[1]} features")
        logger.info("(Would require converting 5K+ sparse features to dense matrix)")
        return None
    
    def train_logistic_regression(self, C=1.0, max_iter=1000):
        """Train Logistic Regression classifier"""
        logger.info(f"Training Logistic Regression (C={C}, max_iter={max_iter})...")
        start_time = time.time()
        
        model = LogisticRegression(
            C=C,
            max_iter=max_iter,
            n_jobs=-1,
            verbose=0,
            random_state=42,
            solver='lbfgs'
        )
        
        model.fit(self.X_train, self.y_train)
        
        elapsed = time.time() - start_time
        logger.info(f"Training completed in {elapsed:.2f} seconds")
        
        self.models['Logistic Regression'] = model
        return model
    
    def evaluate_model(self, model_name, model, top_k_values=[1, 5, 10]):
        """Evaluate model performance"""
        logger.info(f"Evaluating {model_name}...")
        
        # Prepare test data if needed
        if isinstance(self.X_test, sp.spmatrix):
            X_test = self.X_test
        else:
            X_test = self.X_test
        
        # For dense predictions (gradient boosting)
        if hasattr(self.X_test, 'toarray'):
            try:
                y_pred = model.predict(self.X_test.toarray())
            except:
                y_pred = model.predict(self.X_test)
        else:
            y_pred = model.predict(self.X_test)
        
        # Calculate metrics
        accuracy = accuracy_score(self.y_test, y_pred)
        precision = precision_score(self.y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(self.y_test, y_pred, average='weighted', zero_division=0)
        f1 = f1_score(self.y_test, y_pred, average='weighted', zero_division=0)
        
        # Top-k accuracy (only for models with predict_proba and sufficient test classes)
        top_k_accuracies = {}
        try:
            if hasattr(model, 'predict_proba') and len(self.y_test.unique()) > 10:
                y_proba = model.predict_proba(self.X_test.toarray() if hasattr(self.X_test, 'toarray') else self.X_test)
                # Only calculate top-k if we have enough unique classes
                for k in [1, 5]:
                    if k <= len(model.classes_):
                        try:
                            top_k_acc = top_k_accuracy_score(
                                self.y_test, y_proba, k=k,
                                labels=model.classes_
                            )
                            top_k_accuracies[f'top_{k}_accuracy'] = top_k_acc
                        except:
                            pass
        except Exception as e:
            logger.warning(f"Could not calculate top-k accuracy: {e}")
        
        results = {
            'model_name': model_name,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'top_k_accuracies': top_k_accuracies,
            'predictions': y_pred
        }
        
        self.results[model_name] = results
        
        logger.info(f"\n{model_name} Results:")
        logger.info(f"  Accuracy:  {accuracy:.4f}")
        logger.info(f"  Precision: {precision:.4f}")
        logger.info(f"  Recall:    {recall:.4f}")
        logger.info(f"  F1-Score:  {f1:.4f}")
        for k_name, k_acc in top_k_accuracies.items():
            logger.info(f"  {k_name}: {k_acc:.4f}")
        
        return results
    
    def compare_models(self):
        """Compare all trained models"""
        logger.info("\n" + "=" * 80)
        logger.info("MODEL COMPARISON")
        logger.info("=" * 80)
        
        comparison_df = pd.DataFrame([
            {
                'Model': name,
                'Accuracy': results['accuracy'],
                'Precision': results['precision'],
                'Recall': results['recall'],
                'F1-Score': results['f1_score']
            }
            for name, results in self.results.items()
        ])
        
        print("\n" + comparison_df.to_string(index=False))
        
        # Find best model
        best_model = comparison_df.loc[comparison_df['F1-Score'].idxmax()]
        logger.info(f"\nðŸ† Best Model: {best_model['Model']} (F1: {best_model['F1-Score']:.4f})")
        
        return comparison_df
    
    def save_models(self):
        """Save trained models"""
        logger.info("Saving trained models...")
        
        for model_name, model in self.models.items():
            model_path = self.output_dir / f"{model_name.lower().replace(' ', '_')}_model.pkl"
            with open(model_path, 'wb') as f:
                pickle.dump(model, f)
            logger.info(f"Saved: {model_path}")
        
        # Save results
        results_path = self.output_dir / 'model_results.json'
        results_json = {}
        for model_name, results in self.results.items():
            results_json[model_name] = {
                'accuracy': float(results['accuracy']),
                'precision': float(results['precision']),
                'recall': float(results['recall']),
                'f1_score': float(results['f1_score']),
                'top_k_accuracies': {
                    k: float(v) for k, v in results.get('top_k_accuracies', {}).items()
                }
            }
        
        with open(results_path, 'w') as f:
            json.dump(results_json, f, indent=2)
        
        logger.info(f"Saved: {results_path}")
    
    def create_comparison_visualizations(self):
        """Create comparison visualizations"""
        logger.info("Creating comparison visualizations...")
        
        comparison_df = pd.DataFrame([
            {
                'Model': name,
                'Accuracy': results['accuracy'],
                'Precision': results['precision'],
                'Recall': results['recall'],
                'F1-Score': results['f1_score']
            }
            for name, results in self.results.items()
        ])
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Model Comparison - License Classification', fontsize=16, fontweight='bold')
        
        # Metrics comparison
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        x = np.arange(len(comparison_df))
        width = 0.2
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx // 2, idx % 2]
            bars = ax.bar(x, comparison_df[metric], width=0.6, color=['#3498db', '#e74c3c', '#2ecc71', '#f39c12'][:len(comparison_df)])
            ax.set_ylabel('Score')
            ax.set_title(f'{metric}')
            ax.set_xticks(x)
            ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
            ax.set_ylim([0, 1])
            ax.grid(axis='y', alpha=0.3)
            
            # Add value labels on bars
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.3f}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'model_comparison.png', dpi=300, bbox_inches='tight')
        logger.info("Saved: model_comparison.png")
        plt.close()
        
        # Radar chart for detailed comparison
        categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]
        
        colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']
        for idx, row in comparison_df.iterrows():
            values = row[['Accuracy', 'Precision', 'Recall', 'F1-Score']].tolist()
            values += values[:1]
            ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx])
            ax.fill(angles, values, alpha=0.15, color=colors[idx])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_ylim(0, 1)
        ax.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'model_radar_chart.png', dpi=300, bbox_inches='tight')
        logger.info("Saved: model_radar_chart.png")
        plt.close()
    
    def generate_report(self, comparison_df):
        """Generate comprehensive training report"""
        logger.info("Generating training report...")
        
        report = []
        report.append("=" * 100)
        report.append("CLASSIFICATION MODEL TRAINING REPORT")
        report.append("SPDX License Identification Task")
        report.append("=" * 100)
        
        report.append("\nðŸ“‹ TASK OVERVIEW")
        report.append("-" * 100)
        report.append("Objective: Build ML classifiers to identify SPDX licenses from text")
        report.append("Dataset: 718 SPDX licenses (574 train, 144 test)")
        report.append("Features: 5,002 (5,000 TF-IDF + 2 categorical)")
        report.append("Target Classes: 718 unique SPDX license IDs")
        report.append("Class Type: Multi-class (718-class problem)")
        
        report.append("\nðŸ¤– MODELS TRAINED")
        report.append("-" * 100)
        for model_name in self.models.keys():
            report.append(f"  âœ… {model_name}")
        
        report.append("\nðŸ“Š PERFORMANCE COMPARISON")
        report.append("-" * 100)
        report.append("\n" + comparison_df.to_string(index=False))
        
        report.append("\n\nðŸ† BEST MODEL")
        report.append("-" * 100)
        best_idx = comparison_df['F1-Score'].idxmax()
        best_model = comparison_df.loc[best_idx]
        report.append(f"Model: {best_model['Model']}")
        report.append(f"Accuracy:  {best_model['Accuracy']:.4f}")
        report.append(f"Precision: {best_model['Precision']:.4f}")
        report.append(f"Recall:    {best_model['Recall']:.4f}")
        report.append(f"F1-Score:  {best_model['F1-Score']:.4f}")
        
        report.append("\nðŸ’¡ KEY INSIGHTS")
        report.append("-" * 100)
        report.append(f"â€¢ Training samples: {len(self.y_train)}")
        report.append(f"â€¢ Test samples: {len(self.y_test)}")
        report.append(f"â€¢ Unique train classes: {len(self.y_train.unique())}")
        report.append(f"â€¢ Unique test classes: {len(self.y_test.unique())}")
        report.append(f"â€¢ Feature dimensionality: {self.X_train.shape[1]}")
        report.append(f"â€¢ Sparsity: {(1 - self.X_train.nnz / (self.X_train.shape[0] * self.X_train.shape[1])) * 100:.2f}%")
        
        report.append("\nðŸŽ¯ METRICS EXPLANATION")
        report.append("-" * 100)
        report.append("Accuracy: Overall correctness of predictions")
        report.append("Precision: Correct positive predictions / all positive predictions")
        report.append("Recall: Correct positive predictions / actual positives")
        report.append("F1-Score: Harmonic mean of Precision and Recall (0.0 to 1.0)")
        
        report.append("\nðŸ“ OUTPUT FILES")
        report.append("-" * 100)
        report.append("âœ… svm_model.pkl - Trained SVM classifier")
        report.append("âœ… random_forest_model.pkl - Trained Random Forest classifier")
        report.append("âœ… gradient_boosting_model.pkl - Trained Gradient Boosting classifier")
        report.append("âœ… logistic_regression_model.pkl - Trained Logistic Regression classifier")
        report.append("âœ… model_results.json - Detailed results")
        report.append("âœ… model_comparison.png - Bar chart comparison")
        report.append("âœ… model_radar_chart.png - Radar chart visualization")
        report.append("âœ… classification_report.txt - Full training report")
        
        report.append("\nðŸš€ NEXT STEPS")
        report.append("-" * 100)
        report.append("1. Deploy best model for license identification")
        report.append("2. Test on real-world license texts")
        report.append("3. Compare with ScanCode and FOSSology results")
        report.append("4. Fine-tune hyperparameters if needed")
        report.append("5. Implement ensemble methods for improved accuracy")
        report.append("6. Add BERT embeddings for state-of-the-art performance")
        
        report_text = "\n".join(report)
        
        # Save report
        report_path = self.output_dir / 'classification_report.txt'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"Report saved to {report_path}")
        print("\n" + report_text)
        
        return report_text
    
    def run_pipeline(self):
        """Run complete classification training pipeline"""
        logger.info("=" * 100)
        logger.info("STARTING CLASSIFICATION MODEL TRAINING PIPELINE")
        logger.info("=" * 100)
        
        # Load features
        self.load_features()
        
        # Train all models
        logger.info("\n" + "=" * 100)
        logger.info("TRAINING MODELS")
        logger.info("=" * 100 + "\n")
        
        self.train_svm(kernel='rbf', C=1.0)
        self.train_random_forest(n_estimators=100, max_depth=20)
        # self.train_gradient_boosting(n_estimators=100, learning_rate=0.1)  # Too slow
        self.train_logistic_regression(C=1.0, max_iter=1000)
        
        # Evaluate all models
        logger.info("\n" + "=" * 100)
        logger.info("EVALUATING MODELS")
        logger.info("=" * 100 + "\n")
        
        for model_name, model in self.models.items():
            self.evaluate_model(model_name, model)
        
        # Compare models
        comparison_df = self.compare_models()
        
        # Create visualizations
        self.create_comparison_visualizations()
        
        # Save models
        self.save_models()
        
        # Generate report
        self.generate_report(comparison_df)
        
        logger.info("\n" + "=" * 100)
        logger.info("CLASSIFICATION TRAINING PIPELINE COMPLETED!")
        logger.info("=" * 100)
        logger.info(f"\nOutputs saved to: {self.output_dir}")
        
        return self.models, self.results


if __name__ == "__main__":
    features_dir = r"c:\Users\ASUS\Desktop\ML project 2\data\features"
    output_dir = r"c:\Users\ASUS\Desktop\ML project 2\models"
    
    classifier = LicenseClassifier(features_dir, output_dir)
    models, results = classifier.run_pipeline()
    
    logger.info("\nâœ… MODEL TRAINING COMPLETED SUCCESSFULLY!")
    logger.info("Next: Deploy best model or fine-tune hyperparameters")
